{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import random\n",
    "import tarfile\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchtext.vocab as Vocab\n",
    "import torch.utils.data as Data\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import d2lzh_pytorch as d2l\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "DATA_ROOT = \"../IMDb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "从压缩包解压。。。\n"
     ]
    }
   ],
   "source": [
    "#读取数据\n",
    "fname = os.path.join(DATA_ROOT,'aclImdb_v1.tar.gz')\n",
    "if not os.path.exists(os.path.join(DATA_ROOT, \"aclImdb\")):\n",
    "    print('从压缩包解压。。。')\n",
    "    with tarfile.open(fname,'r') as f:\n",
    "        f.extractall(DATA_ROOT)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "接下来，读取训练数据集和测试数据集。每个样本是一条评论及其对应的标签：1表示“正面”，0表示“负面”。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12500/12500 [00:01<00:00, 8398.29it/s]\n",
      "100%|██████████| 12500/12500 [00:00<00:00, 13024.78it/s]\n",
      "100%|██████████| 12500/12500 [00:00<00:00, 14506.27it/s]\n",
      "100%|██████████| 12500/12500 [00:01<00:00, 10025.10it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def read_imdb(folder='train',data_root='\\\\Users\\\\ShuaiqiDuan\\\\Desktop\\\\Research\\\\DL\\\\Dive-into-DL-PyTorch\\\\IMDb\\\\aclImdb'):\n",
    "    data = []\n",
    "    for label in ['pos','neg']:\n",
    "        folder_name = os.path.join(data_root,folder,label)\n",
    "        for file in tqdm(os.listdir(folder_name)):\n",
    "            with open(os.path.join(folder_name,file),'rb') as f:\n",
    "                review = f.read().decode('utf-8').replace('\\n','').lower()\n",
    "                data.append([review,1 if label == 'pos' else 0])\n",
    "    random.shuffle(data)\n",
    "    return data\n",
    "train_data, test_data = read_imdb('train'), read_imdb('test')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "10.7.1.2 预处理数据\n",
    "我们需要对每条评论做分词，从而得到分好词的评论。这里定义的get_tokenized_imdb函数使用最简单的方法：基于空格进行分词。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def get_tokenized_imdb(data):\n",
    "    '''\n",
    "    data: list of [string, label]\n",
    "    :param data:\n",
    "    :return:\n",
    "    '''\n",
    "    def tokenizer(text):\n",
    "        return [tok.lower() for tok in text.split(' ')]\n",
    "    return [tokenizer(review) for review,_ in data]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "现在，我们可以根据分好词的训练数据集来创建词典了。我们在这里过滤掉了出现次数少于5的词。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "46150"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_vocab_imdb(data):\n",
    "    tokenized_data = get_tokenized_imdb(data)\n",
    "    counter = collections.Counter([tk for st in tokenized_data for tk in st])\n",
    "    return Vocab.vocab(counter, min_freq=5)\n",
    "\n",
    "vocab = get_vocab_imdb(train_data)\n",
    "len(vocab)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "因为每条评论长度不一致所以不能直接组合成小批量，我们定义preprocess_imdb函数对每条评论进行分词，并通过词典转换成词索引，然后通过截断或者补0来将每条评论长度固定成500。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "def preprocess_imdb(data, vocab):\n",
    "    max_l = 500  # 将每条评论通过截断或者补0，使得长度变成500\n",
    "\n",
    "    def pad(x):\n",
    "        return x[:max_l] if len(x) > max_l else x + [0] * (max_l - len(x))\n",
    "\n",
    "    tokenized_data = get_tokenized_imdb(data)\n",
    "    features = torch.tensor([pad([vocab.get_stoi().get(word,0) for word in words]) for words in tokenized_data])\n",
    "    labels = torch.tensor([score for _, score in data])\n",
    "    return features, labels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "10.7.1.3 创建数据迭代器\n",
    "现在，我们创建数据迭代器。每次迭代将返回一个小批量的数据。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [27], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m batch_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m64\u001B[39m\n\u001B[1;32m----> 2\u001B[0m train_set \u001B[38;5;241m=\u001B[39m Data\u001B[38;5;241m.\u001B[39mTensorDataset(\u001B[38;5;241m*\u001B[39mpreprocess_imdb(train_data,vocab))\n\u001B[0;32m      3\u001B[0m test_set \u001B[38;5;241m=\u001B[39m Data\u001B[38;5;241m.\u001B[39mTensorDataset(\u001B[38;5;241m*\u001B[39mpreprocess_imdb(test_data,vocab))\n\u001B[0;32m      4\u001B[0m train_iter \u001B[38;5;241m=\u001B[39m Data\u001B[38;5;241m.\u001B[39mDataLoader(train_set,batch_size,shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "Cell \u001B[1;32mIn [25], line 8\u001B[0m, in \u001B[0;36mpreprocess_imdb\u001B[1;34m(data, vocab)\u001B[0m\n\u001B[0;32m      5\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x[:max_l] \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(x) \u001B[38;5;241m>\u001B[39m max_l \u001B[38;5;28;01melse\u001B[39;00m x \u001B[38;5;241m+\u001B[39m [\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m*\u001B[39m (max_l \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mlen\u001B[39m(x))\n\u001B[0;32m      7\u001B[0m tokenized_data \u001B[38;5;241m=\u001B[39m get_tokenized_imdb(data)\n\u001B[1;32m----> 8\u001B[0m features \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor([pad([vocab\u001B[38;5;241m.\u001B[39mget_stoi()\u001B[38;5;241m.\u001B[39mget(word,\u001B[38;5;241m0\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m words]) \u001B[38;5;28;01mfor\u001B[39;00m words \u001B[38;5;129;01min\u001B[39;00m tokenized_data])\n\u001B[0;32m      9\u001B[0m labels \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor([score \u001B[38;5;28;01mfor\u001B[39;00m _, score \u001B[38;5;129;01min\u001B[39;00m data])\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m features, labels\n",
      "Cell \u001B[1;32mIn [25], line 8\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m      5\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x[:max_l] \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(x) \u001B[38;5;241m>\u001B[39m max_l \u001B[38;5;28;01melse\u001B[39;00m x \u001B[38;5;241m+\u001B[39m [\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m*\u001B[39m (max_l \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mlen\u001B[39m(x))\n\u001B[0;32m      7\u001B[0m tokenized_data \u001B[38;5;241m=\u001B[39m get_tokenized_imdb(data)\n\u001B[1;32m----> 8\u001B[0m features \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor([pad([vocab\u001B[38;5;241m.\u001B[39mget_stoi()\u001B[38;5;241m.\u001B[39mget(word,\u001B[38;5;241m0\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m words]) \u001B[38;5;28;01mfor\u001B[39;00m words \u001B[38;5;129;01min\u001B[39;00m tokenized_data])\n\u001B[0;32m      9\u001B[0m labels \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor([score \u001B[38;5;28;01mfor\u001B[39;00m _, score \u001B[38;5;129;01min\u001B[39;00m data])\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m features, labels\n",
      "Cell \u001B[1;32mIn [25], line 8\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m      5\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x[:max_l] \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(x) \u001B[38;5;241m>\u001B[39m max_l \u001B[38;5;28;01melse\u001B[39;00m x \u001B[38;5;241m+\u001B[39m [\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m*\u001B[39m (max_l \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mlen\u001B[39m(x))\n\u001B[0;32m      7\u001B[0m tokenized_data \u001B[38;5;241m=\u001B[39m get_tokenized_imdb(data)\n\u001B[1;32m----> 8\u001B[0m features \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor([pad([\u001B[43mvocab\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_stoi\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mget(word,\u001B[38;5;241m0\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m words]) \u001B[38;5;28;01mfor\u001B[39;00m words \u001B[38;5;129;01min\u001B[39;00m tokenized_data])\n\u001B[0;32m      9\u001B[0m labels \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor([score \u001B[38;5;28;01mfor\u001B[39;00m _, score \u001B[38;5;129;01min\u001B[39;00m data])\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m features, labels\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "train_set = Data.TensorDataset(*preprocess_imdb(train_data,vocab))\n",
    "test_set = Data.TensorDataset(*preprocess_imdb(test_data,vocab))\n",
    "train_iter = Data.DataLoader(train_set,batch_size,shuffle=True)\n",
    "test_iter = Data.DataLoader(test_set,batch_size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class BiRNN(nn.Module):\n",
    "    def __init__(self,vocab,embed_size,num_hiddens,num_layers):\n",
    "        super(BiRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(len(vocab),embed_size)\n",
    "        self.encoder = nn.LSTM(input_size=embed_size,\n",
    "                               hidden_size=num_hiddens,\n",
    "                               num_layers=num_layers,\n",
    "                               bidirectional=True)\n",
    "        #初始时间步和最终时间步的隐藏状态作为全连接层输入\n",
    "        self.decoder = nn.Linear(4*num_hiddens,2)\n",
    "\n",
    "    def forward(self,inputs):\n",
    "          # inputs的形状是(批量大小, 词数)，因为LSTM需要将序列长度(seq_len)作为第一维，所以将输入转置后\n",
    "        # 再提取词特征，输出形状为(词数, 批量大小, 词向量维度)\n",
    "        embeddings = self.embedding(inputs.permute(1, 0))\n",
    "        # rnn.LSTM只传入输入embeddings，因此只返回最后一层的隐藏层在各时间步的隐藏状态。\n",
    "        # outputs形状是(词数, 批量大小, 2 * 隐藏单元个数)\n",
    "        outputs, _ = self.encoder(embeddings) # output, (h, c)\n",
    "        # 连结初始时间步和最终时间步的隐藏状态作为全连接层输入。它的形状为\n",
    "        # (批量大小, 4 * 隐藏单元个数)。\n",
    "        encoding = torch.cat((outputs[0], outputs[-1]), -1)\n",
    "        outs = self.decoder(encoding)\n",
    "        return outs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "embed_size, num_hiddens, num_layers = 100, 100, 2\n",
    "net = BiRNN(vocab, embed_size, num_hiddens, num_layers)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "10.7.2.1 加载预训练的词向量\n",
    "由于情感分类的训练数据集并不是很大，为应对过拟合，我们将直接使用在更大规模语料上预训练的词向量作为每个词的特征向量。这里，我们为词典vocab中的每个词加载100维的GloVe词向量。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "glove_vocab = Vocab.GloVe(name='6B', dim=100, cache=os.path.join(DATA_ROOT, \"glove\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 本函数已保存在d2lzh_torch包中方便以后使用\n",
    "def load_pretrained_embedding(words, pretrained_vocab):\n",
    "    \"\"\"从预训练好的vocab中提取出words对应的词向量\"\"\"\n",
    "    embed = torch.zeros(len(words), pretrained_vocab.vectors[0].shape[0]) # 初始化为0\n",
    "    oov_count = 0 # out of vocabulary\n",
    "    for i, word in enumerate(words):\n",
    "        try:\n",
    "            idx = pretrained_vocab.stoi[word]\n",
    "            embed[i, :] = pretrained_vocab.vectors[idx]\n",
    "        except KeyError:\n",
    "            oov_count += 1\n",
    "    if oov_count > 0:\n",
    "        print(\"There are %d oov words.\" % oov_count)\n",
    "    return embed\n",
    "\n",
    "net.embedding.weight.data.copy_(\n",
    "    load_pretrained_embedding(vocab.itos, glove_vocab))\n",
    "net.embedding.weight.requires_grad = False # 直接加载预训练好的, 所以不需要更新它"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lr, num_epochs = 0.01, 5\n",
    "# 要过滤掉不计算梯度的embedding参数\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=lr)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "d2l.train(train_iter, test_iter, net, loss, optimizer, device, num_epochs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 本函数已保存在d2lzh_pytorch包中方便以后使用\n",
    "def predict_sentiment(net, vocab, sentence):\n",
    "    \"\"\"sentence是词语的列表\"\"\"\n",
    "    device = list(net.parameters())[0].device\n",
    "    sentence = torch.tensor([vocab.stoi[word] for word in sentence], device=device)\n",
    "    label = torch.argmax(net(sentence.view((1, -1))), dim=1)\n",
    "    return 'positive' if label.item() == 1 else 'negative'"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "torch",
   "language": "python",
   "display_name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
