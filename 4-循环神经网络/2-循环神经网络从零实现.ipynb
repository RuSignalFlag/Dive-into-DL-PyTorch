{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import zipfile\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import d2lzh_pytorch as d2l\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def load_data_jay_lyrics():\n",
    "    \"\"\"加载周杰伦歌词数据集\"\"\"\n",
    "    with zipfile.ZipFile('../jaychou_lyrics.txt.zip') as zin:\n",
    "        with zin.open('jaychou_lyrics.txt') as f:\n",
    "            corpus_chars = f.read().decode('utf-8')\n",
    "    corpus_chars = corpus_chars.replace('\\n', ' ').replace('\\r', ' ')\n",
    "    corpus_chars = corpus_chars[0:10000]\n",
    "    idx_to_char = list(set(corpus_chars))\n",
    "    char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])\n",
    "    vocab_size = len(char_to_idx)\n",
    "    corpus_indices = [char_to_idx[char] for char in corpus_chars]\n",
    "    return corpus_indices, char_to_idx, idx_to_char, vocab_size"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "(corpus_indices, char_to_idx, idx_to_char, vocab_size) = load_data_jay_lyrics()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "[990,\n 154,\n 79,\n 682,\n 112,\n 838,\n 420,\n 990,\n 154,\n 440,\n 311,\n 541,\n 445,\n 545,\n 198,\n 8,\n 420,\n 990,\n 154,\n 440,\n 311,\n 437,\n 536,\n 415,\n 859,\n 972,\n 420,\n 437,\n 536,\n 415,\n 545,\n 198,\n 56,\n 420,\n 519,\n 702,\n 101,\n 702,\n 101,\n 702,\n 101,\n 415,\n 990,\n 990,\n 990,\n 990,\n 630,\n 311,\n 420,\n 881,\n 895,\n 521,\n 119,\n 546,\n 420,\n 893,\n 519,\n 253,\n 248,\n 34,\n 866,\n 47,\n 176,\n 348,\n 420,\n 376,\n 834,\n 937,\n 192,\n 428,\n 563,\n 420,\n 893,\n 519,\n 707,\n 445,\n 311,\n 420,\n 413,\n 336,\n 521,\n 893,\n 519,\n 0,\n 303,\n 521,\n 991,\n 930,\n 42,\n 734,\n 420,\n 594,\n 341,\n 521,\n 893,\n 519,\n 192,\n 12,\n 521,\n 991,\n 930,\n 42,\n 734,\n 420,\n 260,\n 942,\n 521,\n 893,\n 519,\n 376,\n 821,\n 521,\n 991,\n 930,\n 42,\n 734,\n 420,\n 201,\n 201,\n 521,\n 893,\n 519,\n 302,\n 410,\n 521,\n 991,\n 930,\n 42,\n 734,\n 420,\n 201,\n 201,\n 521,\n 893,\n 519,\n 302,\n 410,\n 521,\n 991,\n 930,\n 42,\n 734,\n 420,\n 413,\n 336,\n 521,\n 893,\n 519,\n 0,\n 303,\n 521,\n 991,\n 930,\n 42,\n 734,\n 420,\n 594,\n 341,\n 521,\n 893,\n 519,\n 192,\n 12,\n 521,\n 991,\n 930,\n 42,\n 734,\n 420,\n 260,\n 942,\n 521,\n 893,\n 519,\n 376,\n 821,\n 521,\n 991,\n 930,\n 42,\n 734,\n 420,\n 201,\n 201,\n 521,\n 893,\n 519,\n 302,\n 410,\n 521,\n 991,\n 930,\n 42,\n 734,\n 420,\n 201,\n 201,\n 521,\n 893,\n 519,\n 302,\n 410,\n 521,\n 991,\n 930,\n 42,\n 734,\n 420,\n 990,\n 154,\n 79,\n 682,\n 112,\n 838,\n 420,\n 990,\n 154,\n 440,\n 311,\n 541,\n 445,\n 545,\n 198,\n 8,\n 420,\n 990,\n 154,\n 440,\n 311,\n 437,\n 536,\n 415,\n 859,\n 972,\n 420,\n 437,\n 536,\n 415,\n 545,\n 198,\n 56,\n 420,\n 519,\n 702,\n 101,\n 702,\n 101,\n 702,\n 101,\n 415,\n 990,\n 990,\n 990,\n 990,\n 630,\n 311,\n 420,\n 881,\n 895,\n 521,\n 119,\n 546,\n 420,\n 893,\n 519,\n 253,\n 248,\n 34,\n 866,\n 47,\n 176,\n 348,\n 420,\n 376,\n 834,\n 937,\n 192,\n 428,\n 563,\n 420,\n 893,\n 519,\n 707,\n 445,\n 311,\n 420,\n 413,\n 336,\n 521,\n 893,\n 519,\n 0,\n 303,\n 521,\n 991,\n 930,\n 42,\n 734,\n 420,\n 594,\n 341,\n 521,\n 893,\n 519,\n 192,\n 12,\n 521,\n 991,\n 930,\n 42,\n 734,\n 420,\n 260,\n 942,\n 521,\n 893,\n 519,\n 376,\n 821,\n 521,\n 991,\n 930,\n 42,\n 734,\n 420,\n 201,\n 201,\n 521,\n 893,\n 519,\n 302,\n 410,\n 521,\n 991,\n 930,\n 42,\n 734,\n 420,\n 201,\n 201,\n 521,\n 893,\n 519,\n 302,\n 410,\n 521,\n 991,\n 930,\n 42,\n 734,\n 420,\n 413,\n 336,\n 521,\n 893,\n 519,\n 0,\n 303,\n 521,\n 991,\n 930,\n 42,\n 734,\n 420,\n 594,\n 341,\n 521,\n 893,\n 519,\n 192,\n 12,\n 521,\n 991,\n 930,\n 42,\n 734,\n 420,\n 260,\n 942,\n 521,\n 893,\n 519,\n 376,\n 821,\n 521,\n 991,\n 930,\n 42,\n 734,\n 420,\n 201,\n 201,\n 521,\n 893,\n 519,\n 302,\n 410,\n 521,\n 991,\n 930,\n 42,\n 734,\n 420,\n 201,\n 201,\n 521,\n 893,\n 519,\n 302,\n 410,\n 521,\n 991,\n 930,\n 42,\n 734,\n 420,\n 413,\n 336,\n 521,\n 893,\n 519,\n 0,\n 303,\n 521,\n 991,\n 930,\n 42,\n 734,\n 420,\n 594,\n 341,\n 521,\n 893,\n 519,\n 192,\n 12,\n 521,\n 991,\n 930,\n 42,\n 734,\n 420,\n 260,\n 942,\n 521,\n 893,\n 519,\n 376,\n 821,\n 521,\n 991,\n 930,\n 42,\n 734,\n 420,\n 201,\n 201,\n 521,\n 893,\n 519,\n 302,\n 410,\n 521,\n 991,\n 930,\n 42,\n 734,\n 420,\n 201,\n 201,\n 521,\n 893,\n 519,\n 302,\n 410,\n 521,\n 991,\n 930,\n 42,\n 734,\n 420,\n 413,\n 336,\n 521,\n 893,\n 519,\n 0,\n 303,\n 521,\n 991,\n 930,\n 42,\n 734,\n 420,\n 594,\n 341,\n 521,\n 893,\n 519,\n 192,\n 12,\n 521,\n 991,\n 930,\n 42,\n 734,\n 420,\n 260,\n 942,\n 521,\n 893,\n 519,\n 376,\n 821,\n 521,\n 991,\n 930,\n 42,\n 734,\n 420,\n 201,\n 201,\n 521,\n 893,\n 519,\n 302,\n 410,\n 521,\n 991,\n 930,\n 42,\n 734,\n 420,\n 201,\n 201,\n 521,\n 893,\n 519,\n 302,\n 410,\n 521,\n 991,\n 930,\n 42,\n 734,\n 420,\n 888,\n 300,\n 667,\n 225,\n 909,\n 420,\n 991,\n 857,\n 889,\n 604,\n 888,\n 300,\n 667,\n 976,\n 130,\n 420,\n 301,\n 614,\n 941,\n 191,\n 420,\n 888,\n 300,\n 881,\n 859,\n 487,\n 420,\n 139,\n 521,\n 991,\n 857,\n 420,\n 519,\n 990,\n 154,\n 277,\n 519,\n 521,\n 506,\n 314,\n 571,\n 749,\n 420,\n 584,\n 843,\n 415,\n 881,\n 56,\n 420,\n 609,\n 492,\n 229,\n 610,\n 420,\n 584,\n 843,\n 277,\n 760,\n 8,\n 420,\n 499,\n 499,\n 594,\n 414,\n 420,\n 893,\n 519,\n 930,\n 522,\n 311,\n 420,\n 500,\n 879,\n 94,\n 915,\n 420,\n 438,\n 311,\n 946,\n 591,\n 323,\n 238,\n 521,\n 859,\n 879,\n 718,\n 420,\n 159,\n 429,\n 192,\n 179,\n 392,\n 880,\n 420,\n 581,\n 193,\n 193,\n 938,\n 435,\n 420,\n 311,\n 930,\n 760,\n 519,\n 521,\n 2,\n 688,\n 420,\n 893,\n 852,\n 537,\n 521,\n 478,\n 494,\n 420,\n 416,\n 84,\n 25,\n 153,\n 373,\n 420,\n 75,\n 734,\n 521,\n 591,\n 655,\n 420,\n 311,\n 521,\n 946,\n 591,\n 103,\n 746,\n 420,\n 754,\n 114,\n 631,\n 420,\n 893,\n 519,\n 774,\n 913,\n 662,\n 138,\n 857,\n 816,\n 434,\n 420,\n 277,\n 139,\n 192,\n 1019,\n 130,\n 896,\n 25,\n 229,\n 435,\n 420,\n 254,\n 438,\n 859,\n 879,\n 48,\n 915,\n 420,\n 311,\n 521,\n 946,\n 591,\n 103,\n 746,\n 420,\n 754,\n 114,\n 631,\n 420,\n 976,\n 131,\n 521,\n 855,\n 254,\n 903,\n 651,\n 234,\n 563,\n 420,\n 519,\n 380,\n 291,\n 301,\n 340,\n 563,\n 581,\n 81,\n 972,\n 420,\n 748,\n 987,\n 90,\n 37,\n 809,\n 759,\n 420,\n 584,\n 843,\n 415,\n 881,\n 56,\n 420,\n 609,\n 492,\n 229,\n 610,\n 420,\n 584,\n 843,\n 277,\n 760,\n 8,\n 420,\n 499,\n 499,\n 594,\n 414,\n 420,\n 893,\n 519,\n 930,\n 522,\n 311,\n 420,\n 500,\n 879,\n 94,\n 915,\n 420,\n 438,\n 311,\n 946,\n 591,\n 323,\n 238,\n 521,\n 859,\n 879,\n 718,\n 420,\n 159,\n 429,\n 192,\n 179,\n 392,\n 880,\n 420,\n 581,\n 193,\n 193,\n 938,\n 435,\n 420,\n 311,\n 930,\n 760,\n 519,\n 521,\n 2,\n 688,\n 420,\n 893,\n 852,\n 537,\n 521,\n 478,\n 494,\n 420,\n 416,\n 84,\n 25,\n 153,\n 373,\n 420,\n 75,\n 734,\n 521,\n 591,\n 655,\n 420,\n 311,\n 521,\n 946,\n 591,\n 103,\n 746,\n 420,\n 754,\n 114,\n 631,\n 420,\n 893,\n 519,\n 774,\n 913,\n 662,\n 138,\n 857,\n 816,\n 434,\n 420,\n 277,\n 139,\n 192,\n 1019,\n 130,\n 896,\n 25,\n 229,\n 435,\n 420,\n 254,\n 438,\n 859,\n 879,\n 48,\n 915,\n 420,\n 311,\n 521,\n 946,\n 591,\n 103,\n 746,\n 420,\n 754,\n 114,\n 631,\n 420,\n 976,\n 131,\n 521,\n 855,\n 254,\n 903,\n 651,\n 234,\n 563,\n 420,\n 519,\n 380,\n 291,\n 301,\n 340,\n 563,\n 581,\n 81,\n 972,\n 420,\n 748,\n 987,\n 90,\n 37,\n 809,\n 759,\n 420,\n 730,\n 239,\n 185,\n 420,\n 730,\n 239,\n 185,\n 420,\n 859,\n 622,\n 498,\n 622,\n 794,\n 622,\n 869,\n 622,\n 993,\n 630,\n 101,\n 420,\n 30,\n 835,\n 835,\n 420,\n 859,\n 46,\n 498,\n 46,\n 794,\n 46,\n 869,\n 46,\n 420,\n 774,\n 25,\n 553,\n 859,\n 622,\n 498,\n 622,\n 794,\n 622,\n 869,\n 622,\n 993,\n 630,\n 101,\n 420,\n 30,\n 835,\n 835,\n 420,\n 859,\n 46,\n 498,\n 46,\n 794,\n 46,\n 869,\n 46,\n 420,\n 774,\n 25,\n 553,\n 99,\n 630,\n 350,\n 420,\n 849,\n 779,\n 415,\n 240,\n 101,\n 482,\n 420,\n 859,\n 561,\n 744,\n 733,\n 215,\n 415,\n 519,\n 0,\n 853,\n 420,\n 965,\n 25,\n 311,\n 521,\n 865,\n 274,\n ...]"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_indices"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "1027"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "为了将词表示成向量输入到神经网络，一个简单的办法是使用one-hot向量。假设词典中不同字符的数量为NN（即词典大小vocab_size），每个字符已经同一个从0到N−1N−1的连续整数值索引一一对应。如果一个字符的索引是整数ii, 那么我们创建一个全0的长为NN的向量，并将其位置为ii的元素设成1。该向量就是对原字符的one-hot向量。下面分别展示了索引为0和2的one-hot向量，向量长度等于词典大小。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 1.,  ..., 0., 0., 0.]])"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def one_hot(x,n_class,dtype = torch.float32):\n",
    "    #X SHAPE BATCH   OUTPUT SHAPE  BATCH,N-CLASS\n",
    "\n",
    "    x = x.long()\n",
    "    res = torch.zeros(x.shape[0],n_class,dtype=dtype,device=x.device)\n",
    "    res.scatter_(1, x.view(-1, 1), 1)\n",
    "    return res\n",
    "\n",
    "x = torch.tensor([0, 2])\n",
    "\n",
    "one_hot(x, vocab_size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2])"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0],\n        [2]])"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x.view(-1,1)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们每次采样的小批量的形状是(批量大小, 时间步数)。下面的函数将这样的小批量变换成数个可以输入进网络的形状为(批量大小, 词典大小)的矩阵，矩阵个数等于时间步数。也就是说，时间步tt的输入为Xt∈Rn×dX\n",
    "t\n",
    "​\n",
    " ∈R\n",
    "n×d\n",
    " ，其中nn为批量大小，dd为输入个数，即one-hot向量长度（词典大小）。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "def to_onehot(X,n_class):\n",
    "    #X shape:(batch,seq_len) output shape: seq_len elements of (batch,n_class)\n",
    "    return [one_hot(X[:,i],n_class) for i in range(X.shape[1])]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0, 1, 2, 3, 4],\n        [5, 6, 7, 8, 9]])"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(10).view(2, 5)\n",
    "\n",
    "\n",
    "X"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 5])"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "5"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape[1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 torch.Size([2, 1027])\n"
     ]
    }
   ],
   "source": [
    "inputs = to_onehot(X, vocab_size)\n",
    "print(len(inputs), inputs[0].shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "初始化模型参数,接下来，我们初始化模型参数。隐藏单元个数 num_hiddens是一个超参数。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use cuda\n"
     ]
    }
   ],
   "source": [
    "num_inputs,num_hiddens,num_outputs = vocab_size,256,vocab_size\n",
    "print('use',device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "def get_params():\n",
    "    def _one(shape):\n",
    "        ts = torch.tensor(np.random.normal(0,0.01,size=shape),device=device,dtype=torch.float32)\n",
    "        return torch.nn.Parameter(ts,requires_grad=True)\n",
    "\n",
    "    #隐藏层参数\n",
    "    W_xh = _one((num_inputs,num_hiddens))\n",
    "    W_hh = _one((num_hiddens,num_hiddens))\n",
    "    b_h = torch.nn.Parameter(torch.zeros(num_hiddens,device=device,requires_grad=True))\n",
    "\n",
    "    #输出层参数\n",
    "    W_hq = _one((num_hiddens,num_outputs))\n",
    "    b_q = torch.nn.Parameter(torch.zeros(num_outputs,device=device,requires_grad=True))\n",
    "    return nn.ParameterList([W_xh,W_hh,b_h,W_hq,b_q])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "定义模型我们根据循环神经网络的计算表达式实现该模型。首先定义init_rnn_state函数来返回初始化的隐藏状态。它返回由一个形状为(批量大小, 隐藏单元个数)的值为0的NDArray组成的元组。使用元组是为了更便于处理隐藏状态含有多个NDArray的情况。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "def init_rnn_state(batch_size,num_hiddens,device):\n",
    "    return (torch.zeros((batch_size,num_hiddens),device=device))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "下面的rnn函数定义了在一个时间步里如何计算隐藏状态和输出。这里的激活函数使用了tanh函数。3.8节（多层感知机）中介绍过，当元素在实数域上均匀分布时，tanh函数值的均值为0。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 torch.Size([2, 1027]) torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "def rnn(inputs,state,params):\n",
    "    #inputs和outputs均为num_steps个形状为（batch_size,vocab_size)的矩阵\n",
    "    W_xh,W_hh,b_h,W_hq,b_q = params\n",
    "    H = state\n",
    "    outputs = []\n",
    "    for x in inputs:\n",
    "        H = torch.tanh(torch.matmul(x,W_xh)+torch.matmul(H,W_hh)+b_h)\n",
    "        Y = torch.matmul(H,W_hq) + b_q\n",
    "        outputs.append(Y)\n",
    "    return outputs,H\n",
    "state = init_rnn_state(X.shape[0], num_hiddens, device)\n",
    "inputs = to_onehot(X.to(device), vocab_size)\n",
    "params = get_params()\n",
    "outputs, state_new = rnn(inputs, state, params)\n",
    "print(len(outputs), outputs[0].shape, state_new[0].shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "定义预测函数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "以下函数基于前缀prefix（含有数个字符的字符串）来预测接下来的num_chars个字符。这个函数稍显复杂，其中我们将循环神经单元rnn设置成了函数参数，这样在后面小节介绍其他循环神经网络时能重复使用这个函数。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "'分开颁随爽问师么狗们廓靠'"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pred_rnn(prefix,num_chars,rnn,params,init_rnn_state,num_hiddens,vocab_size,device,idx_to_char,char_to_idx):\n",
    "    state = init_rnn_state(1,num_hiddens,device)\n",
    "    output = [char_to_idx[prefix[0]]]\n",
    "    for t in range(num_chars+len(prefix) - 1):\n",
    "        #将上一实时间步的输出作为当前时间步的输入\n",
    "        X = to_onehot(torch.tensor([[output[-1]]],device=device),vocab_size)\n",
    "        #计算输出和更新隐藏状态\n",
    "        (Y,state) = rnn(X,state,params)\n",
    "        if t < len(prefix) - 1:\n",
    "            output.append(char_to_idx[prefix[t + 1]])\n",
    "        else:\n",
    "            output.append(int(Y[0].argmax(dim=1).item()))\n",
    "    return ''.join([idx_to_char[i] for i in output])\n",
    "\n",
    "pred_rnn('分开', 10, rnn, params, init_rnn_state, num_hiddens, vocab_size,\n",
    "            device, idx_to_char, char_to_idx)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "裁剪梯度循环神经网络中较容易出现梯度衰减或梯度爆炸。我们会在6.6节（通过时间反向传播）中解释原因。为了应对梯度爆炸，我们可以裁剪梯度（clip gradient）。假设我们把所有模型参数梯度的元素拼接成一个向量 gg，并设裁剪的阈值是θθ。裁剪后的梯度\n",
    "\n",
    "min(θ∥g∥,1)gmin(\n",
    "∥g∥\n",
    "θ\n",
    "​\n",
    " ,1)g\n",
    "的L2L\n",
    "2\n",
    "​\n",
    " 范数不超过θθ。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "def grad_clipping(params, theta, device):\n",
    "    norm = torch.tensor([0.0], device=device)\n",
    "    for param in params:\n",
    "        norm += (param.grad.data ** 2).sum()\n",
    "    norm = norm.sqrt().item()\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad.data *= (theta / norm)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "6.4.6 困惑度\n",
    "我们通常使用困惑度（perplexity）来评价语言模型的好坏。回忆一下3.4节（softmax回归）中交叉熵损失函数的定义。困惑度是对交叉熵损失函数做指数运算后得到的值。特别地，\n",
    "\n",
    "最佳情况下，模型总是把标签类别的概率预测为1，此时困惑度为1；\n",
    "最坏情况下，模型总是把标签类别的概率预测为0，此时困惑度为正无穷；\n",
    "基线情况下，模型总是预测所有类别的概率都相同，此时困惑度为类别个数。\n",
    "显然，任何一个有效模型的困惑度必须小于类别个数。在本例中，困惑度必须小于词典大小vocab_size。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "跟之前章节的模型训练函数相比，这里的模型训练函数有以下几点不同：\n",
    "\n",
    "使用困惑度评价模型。\n",
    "在迭代模型参数前裁剪梯度。\n",
    "对时序数据采用不同采样方法将导致隐藏状态初始化的不同。相关讨论可参考6.3节（语言模型数据集（周杰伦专辑歌词））。\n",
    "另外，考虑到后面将介绍的其他循环神经网络，为了更通用，这里的函数实现更长一些。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50, perplexity 70.215869, time 0.12 sec\n",
      " - 分开 我想你的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人 坏\n",
      " - 不分开  有有你有你的让我 我有你你 你有我有 你有你的手  哼有你的让我有动的可爱女人 坏坏的让我疯狂的\n",
      "epoch 100, perplexity 9.689514, time 0.24 sec\n",
      " - 分开 我不好好生你 当知不觉 我已经这节奏 后知不觉 你已经这开我 不知不觉 我已经这节奏 后知不觉 你\n",
      " - 不分开永 我不能再想你 我知道这样球 你的完美主义 它彻后 我手无 我想就这样牵着你的手不放开 爱能不能够\n",
      "epoch 150, perplexity 2.923245, time 0.16 sec\n",
      " - 分开 有有那里默 的话都著口 不知不觉 你一经离开 白静蜡美 全暖怕日出 白色蜡烛 温暖了空屋 白色蜡烛\n",
      " - 不分开吗 我后你 别你我 我想就这样牵着你的手不放开 爱可不能够永远单纯没有悲哀 我 想带你骑单车 我 想\n",
      "epoch 200, perplexity 1.589947, time 0.32 sec\n",
      " - 分开 一使心霜的仪斑 我绪店旁二 三两银够不够 景色入秋 漫天黄沙凉寞 哼哼哈兮 如果我有轻功 飞檐走壁\n",
      " - 不分开期 然后将过去 慢慢温习 让我爱上你 那场的剧 是你耿虹 你一定纵我妈大  古前的教育著河的家庭 没\n",
      "epoch 250, perplexity 1.309939, time 0.09 sec\n",
      " - 分开 出在它沙去的豆瓣酱 我 伤物 静战螂 一直怎么三步四步望著天 看星星 一颗两颗三颗四颗 连成线一著\n",
      " - 不分开扫 我叫你爸 你打我妈 这样对吗干嘛这样 何必让它牵鼻子走 瞎 说狼三木斯中出进难现 泥板上的字迹依\n"
     ]
    }
   ],
   "source": [
    "# 本函数已保存在d2lzh_pytorch包中方便以后使用\n",
    "def train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,\n",
    "                          vocab_size, device, corpus_indices, idx_to_char,\n",
    "                          char_to_idx, is_random_iter, num_epochs, num_steps,\n",
    "                          lr, clipping_theta, batch_size, pred_period,\n",
    "                          pred_len, prefixes):\n",
    "    if is_random_iter:\n",
    "        data_iter_fn = d2l.data_iter_random\n",
    "    else:\n",
    "        data_iter_fn = d2l.data_iter_consecutive\n",
    "    params = get_params()\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        if not is_random_iter:  # 如使用相邻采样，在epoch开始时初始化隐藏状态\n",
    "            state = init_rnn_state(batch_size, num_hiddens, device)\n",
    "        l_sum, n, start = 0.0, 0, time.time()\n",
    "        data_iter = data_iter_fn(corpus_indices, batch_size, num_steps, device)\n",
    "        for X, Y in data_iter:\n",
    "            if is_random_iter:  # 如使用随机采样，在每个小批量更新前初始化隐藏状态\n",
    "                state = init_rnn_state(batch_size, num_hiddens, device)\n",
    "            else:\n",
    "            # 否则需要使用detach函数从计算图分离隐藏状态, 这是为了\n",
    "            # 使模型参数的梯度计算只依赖一次迭代读取的小批量序列(防止梯度计算开销太大)\n",
    "                for s in state:\n",
    "                    s.detach_()\n",
    "\n",
    "\n",
    "            inputs = to_onehot(X, vocab_size)\n",
    "            # outputs有num_steps个形状为(batch_size, vocab_size)的矩阵\n",
    "            (outputs, state) = rnn(inputs, state, params)\n",
    "            # 拼接之后形状为(num_steps * batch_size, vocab_size)\n",
    "            outputs = torch.cat(outputs, dim=0)\n",
    "            # Y的形状是(batch_size, num_steps)，转置后再变成长度为\n",
    "            # batch * num_steps 的向量，这样跟输出的行一一对应\n",
    "            y = torch.transpose(Y, 0, 1).contiguous().view(-1)\n",
    "            # 使用交叉熵损失计算平均分类误差\n",
    "            l = loss(outputs, y.long())\n",
    "\n",
    "            # 梯度清0\n",
    "            if params[0].grad is not None:\n",
    "                for param in params:\n",
    "                    param.grad.data.zero_()\n",
    "            l.backward()\n",
    "            grad_clipping(params, clipping_theta, device)  # 裁剪梯度\n",
    "            d2l.sgd(params, lr, 1)  # 因为误差已经取过均值，梯度不用再做平均\n",
    "            l_sum += l.item() * y.shape[0]\n",
    "            n += y.shape[0]\n",
    "\n",
    "        if (epoch + 1) % pred_period == 0:\n",
    "            print('epoch %d, perplexity %f, time %.2f sec' % (\n",
    "                epoch + 1, math.exp(l_sum / n), time.time() - start))\n",
    "            for prefix in prefixes:\n",
    "                print(' -', pred_rnn(prefix, pred_len, rnn, params, init_rnn_state,\n",
    "                    num_hiddens, vocab_size, device, idx_to_char, char_to_idx))\n",
    "\n",
    "\n",
    "num_epochs, num_steps, batch_size, lr, clipping_theta = 250, 35, 32, 1e2, 1e-2\n",
    "pred_period, pred_len, prefixes = 50, 50, ['分开', '不分开']\n",
    "\n",
    "train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,\n",
    "                      vocab_size, device, corpus_indices, idx_to_char,\n",
    "                      char_to_idx, True, num_epochs, num_steps, lr,\n",
    "                      clipping_theta, batch_size, pred_period, pred_len,\n",
    "                      prefixes)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't detach views in-place. Use detach() instead. If you are using DistributedDataParallel (DDP) for training, and gradient_as_bucket_view is set as True, gradients are views of DDP buckets, and hence detach_() cannot be called on these gradients. To fix this error, please refer to the Optimizer.zero_grad() function in torch/optim/optimizer.py as the solution.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [99], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,\n\u001B[0;32m      2\u001B[0m                       vocab_size, device, corpus_indices, idx_to_char,\n\u001B[0;32m      3\u001B[0m                       char_to_idx, \u001B[38;5;28;01mFalse\u001B[39;00m, num_epochs, num_steps, lr,\n\u001B[0;32m      4\u001B[0m                       clipping_theta, batch_size, pred_period, pred_len,\n\u001B[0;32m      5\u001B[0m                       prefixes)\n",
      "Cell \u001B[1;32mIn [98], line 26\u001B[0m, in \u001B[0;36mtrain_and_predict_rnn\u001B[1;34m(rnn, get_params, init_rnn_state, num_hiddens, vocab_size, device, corpus_indices, idx_to_char, char_to_idx, is_random_iter, num_epochs, num_steps, lr, clipping_theta, batch_size, pred_period, pred_len, prefixes)\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:  \n\u001B[0;32m     23\u001B[0m \u001B[38;5;66;03m# 否则需要使用detach函数从计算图分离隐藏状态, 这是为了\u001B[39;00m\n\u001B[0;32m     24\u001B[0m \u001B[38;5;66;03m# 使模型参数的梯度计算只依赖一次迭代读取的小批量序列(防止梯度计算开销太大)\u001B[39;00m\n\u001B[0;32m     25\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m state:\n\u001B[1;32m---> 26\u001B[0m         \u001B[43ms\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdetach_\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     29\u001B[0m inputs \u001B[38;5;241m=\u001B[39m to_onehot(X, vocab_size)\n\u001B[0;32m     30\u001B[0m \u001B[38;5;66;03m# outputs有num_steps个形状为(batch_size, vocab_size)的矩阵\u001B[39;00m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Can't detach views in-place. Use detach() instead. If you are using DistributedDataParallel (DDP) for training, and gradient_as_bucket_view is set as True, gradients are views of DDP buckets, and hence detach_() cannot be called on these gradients. To fix this error, please refer to the Optimizer.zero_grad() function in torch/optim/optimizer.py as the solution."
     ]
    }
   ],
   "source": [
    "train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,\n",
    "                      vocab_size, device, corpus_indices, idx_to_char,\n",
    "                      char_to_idx, False, num_epochs, num_steps, lr,\n",
    "                      clipping_theta, batch_size, pred_period, pred_len,\n",
    "                      prefixes)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "torch",
   "language": "python",
   "display_name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
